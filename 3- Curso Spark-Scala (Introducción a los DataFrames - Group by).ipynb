{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "357a7e67",
   "metadata": {},
   "source": [
    "## Crear DataFrames\n",
    "\n",
    "Aquí aplicaremos la creación de DataFrames que se vió en el anterior Notebook, a modo de ejemplo para ver el uso y funcionamiento de diferentes funciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "270480d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://ALC-1NJW5D3.usersad.everis.int:4041\n",
       "SparkContext available as 'sc' (version = 3.3.0, master = local[*], app id = local-1656667631263)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "defined class Department\r\n",
       "defined class Employee\r\n",
       "defined class DepartmentWithEmployees\r\n",
       "department1: Department = Department(123456,Computer Science)\r\n",
       "department2: Department = Department(789012,Mechanical Engineering)\r\n",
       "department3: Department = Department(345678,Theater and Drama)\r\n",
       "department4: Department = Department(901234,Indoor Recreation)\r\n",
       "employee1: Employee = Employee(michael,armbrust,no-reply@berkeley.edu,100000)\r\n",
       "employee2: Employee = Employee(xiangrui,meng,no-reply@stanford.edu,120000)\r\n",
       "employee3: Employee = Employee(matei,null,no-reply@waterloo.edu,140000)\r\n",
       "employee4: Employee = Employee(null,wendell,no-reply@princeton.edu,160000)\r\n",
       "employee5: Employee = Employee(michael,jackson,no-reply@neverla.nd,80000)\r\n",
       "departmentWithEmployees1: DepartmentWithEmployees = DepartmentWithEmp...\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/01 11:27:22 WARN ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped\r\n"
     ]
    }
   ],
   "source": [
    "// Create the case classes for our domain\n",
    "case class Department(id: String, name: String)\n",
    "case class Employee(firstName: String, lastName: String, email: String, salary: Int)\n",
    "case class DepartmentWithEmployees(department: Department, employees: Seq[Employee])\n",
    "\n",
    "// Create the Departments\n",
    "val department1 = new Department(\"123456\", \"Computer Science\")\n",
    "val department2 = new Department(\"789012\", \"Mechanical Engineering\")\n",
    "val department3 = new Department(\"345678\", \"Theater and Drama\")\n",
    "val department4 = new Department(\"901234\", \"Indoor Recreation\")\n",
    "\n",
    "// Create the Employees\n",
    "val employee1 = new Employee(\"michael\", \"armbrust\", \"no-reply@berkeley.edu\", 100000)\n",
    "val employee2 = new Employee(\"xiangrui\", \"meng\", \"no-reply@stanford.edu\", 120000)\n",
    "val employee3 = new Employee(\"matei\", null, \"no-reply@waterloo.edu\", 140000)\n",
    "val employee4 = new Employee(null, \"wendell\", \"no-reply@princeton.edu\", 160000)\n",
    "val employee5 = new Employee(\"michael\", \"jackson\", \"no-reply@neverla.nd\", 80000)\n",
    "\n",
    "// Create the DepartmentWithEmployees instances from Departments and Employees\n",
    "val departmentWithEmployees1 = new DepartmentWithEmployees(department1, Seq(employee1, employee2))\n",
    "val departmentWithEmployees2 = new DepartmentWithEmployees(department2, Seq(employee3, employee4))\n",
    "val departmentWithEmployees3 = new DepartmentWithEmployees(department3, Seq(employee5, employee4))\n",
    "val departmentWithEmployees4 = new DepartmentWithEmployees(department4, Seq(employee2, employee3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0609f0",
   "metadata": {},
   "source": [
    "### Crear DataFrames a partir de una lista de las clases de casos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8c67bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|          department|           employees|\n",
      "+--------------------+--------------------+\n",
      "|{123456, Computer...|[{michael, armbru...|\n",
      "|{789012, Mechanic...|[{matei, null, no...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "+--------------------+--------------------+\n",
      "|          department|           employees|\n",
      "+--------------------+--------------------+\n",
      "|{345678, Theater ...|[{michael, jackso...|\n",
      "|{901234, Indoor R...|[{xiangrui, meng,...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "departmentsWithEmployeesSeq1: Seq[DepartmentWithEmployees] = List(DepartmentWithEmployees(Department(123456,Computer Science),List(Employee(michael,armbrust,no-reply@berkeley.edu,100000), Employee(xiangrui,meng,no-reply@stanford.edu,120000))), DepartmentWithEmployees(Department(789012,Mechanical Engineering),List(Employee(matei,null,no-reply@waterloo.edu,140000), Employee(null,wendell,no-reply@princeton.edu,160000))))\r\n",
       "df1: org.apache.spark.sql.DataFrame = [department: struct<id: string, name: string>, employees: array<struct<firstName:string,lastName:string,email:string,salary:int>>]\r\n",
       "departmentsWithEmployeesSeq2: Seq[DepartmentWithEmployees] = List(DepartmentWithEmployees(Department(345678,Theater and Drama),List(Employee(michael,jackson,no-reply@neverla.nd,80000), Employee(null,wende...\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val departmentsWithEmployeesSeq1 = Seq(departmentWithEmployees1, departmentWithEmployees2)\n",
    "val df1 = departmentsWithEmployeesSeq1.toDF()\n",
    "df1.show()\n",
    "\n",
    "val departmentsWithEmployeesSeq2 = Seq(departmentWithEmployees3, departmentWithEmployees4)\n",
    "val df2 = departmentsWithEmployeesSeq2.toDF()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12e61b5",
   "metadata": {},
   "source": [
    "# Trabajar con DataFrames\n",
    "## Unión de dos DataFrames\n",
    "\n",
    "En esta sección veremos como unir dos dataframes para trabajar con ellos como si de uno solo se tratara."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b8ff4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|          department|           employees|\n",
      "+--------------------+--------------------+\n",
      "|{123456, Computer...|[{michael, armbru...|\n",
      "|{789012, Mechanic...|[{matei, null, no...|\n",
      "|{345678, Theater ...|[{michael, jackso...|\n",
      "|{901234, Indoor R...|[{xiangrui, meng,...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "unionDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [department: struct<id: string, name: string>, employees: array<struct<firstName:string,lastName:string,email:string,salary:int>>]\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val unionDF = df1.union(df2)\n",
    "unionDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823833ec",
   "metadata": {},
   "source": [
    "# Group by\n",
    "Al igual que la cláusula \"GROUP BY\" de SQL, la función groupBy() de Spark se utiliza para reunir los datos idénticos en grupos en DataFrame/Dataset y realizar funciones de agregación en los datos agrupados. En este artículo, explicaré varios ejemplos de ```groupBy()``` con el lenguaje Scala.\n",
    "\n",
    "El mismo enfoque se puede utilizar con Pyspark (Spark con Python)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9e5fcd",
   "metadata": {},
   "source": [
    "Sintaxis:\n",
    "```groupBy(col1 : scala.Predef.String, cols : scala.Predef.String*) :\n",
    "      org.apache.spark.sql.RelationalGroupedDataset```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60437ea7",
   "metadata": {},
   "source": [
    "Cuando realizamos ```groupBy()``` en Spark Dataframe, devuelve el objeto RelationalGroupedDataset que contiene las siguientes funciones de agregación.\n",
    "\n",
    "```count()``` - Devuelve el recuento de filas de cada grupo.\n",
    "\n",
    "```mean()``` - Devuelve la media de los valores de cada grupo.\n",
    "\n",
    "```max()``` - Devuelve el máximo de los valores de cada grupo.\n",
    "\n",
    "```min()``` - Devuelve el mínimo de los valores de cada grupo.\n",
    "\n",
    "```sum()``` - Devuelve el total de los valores de cada grupo.\n",
    "\n",
    "```avg()``` - Devuelve la media de los valores de cada grupo.\n",
    "\n",
    "```agg()``` - Utilizando la función agg(), podemos calcular más de un agregado a la vez.\n",
    "\n",
    "```pivot()``` - Esta función se utiliza para pivotar el DataFrame que se verá más en profundidad más adelante."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f06d49",
   "metadata": {},
   "source": [
    "## Preparando los datos y el DataFrame\n",
    "Antes de empezar, vamos a crear el DataFrame a partir de una secuencia de los datos con los que vamos a trabajar. Este DataFrame contiene las columnas \"employee_name\", \"department\", \"state\", \"salary\", \"age\" y \"bonus\".\n",
    "\n",
    "Utilizaremos este DataFrame de Spark para ejecutar ```groupBy()``` en las columnas \"department\" y calcular los agregados como mínimo, máximo, promedio, salario total para cada grupo utilizando las funciones de agregación ```min()```, ```max()``` y ```sum()``` respectivamente. y finalmente, también veremos cómo hacer grupos y agregados en múltiples columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4871523d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://ALC-1NJW5D3.usersad.everis.int:4040\n",
       "SparkContext available as 'sc' (version = 3.3.0, master = local[*], app id = local-1656916598302)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/04 08:36:55 WARN ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|        James|     Sales|   NY| 90000| 34|10000|\n",
      "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
      "|       Robert|     Sales|   CA| 81000| 30|23000|\n",
      "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
      "|        Raman|   Finance|   CA| 99000| 40|24000|\n",
      "|        Scott|   Finance|   NY| 83000| 36|19000|\n",
      "|          Jen|   Finance|   NY| 79000| 53|15000|\n",
      "|         Jeff| Marketing|   CA| 80000| 25|18000|\n",
      "|        Kumar| Marketing|   NY| 91000| 50|21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\r\n",
       "simpleData: Seq[(String, String, String, Int, Int, Int)] = List((James,Sales,NY,90000,34,10000), (Michael,Sales,NY,86000,56,20000), (Robert,Sales,CA,81000,30,23000), (Maria,Finance,CA,90000,24,23000), (Raman,Finance,CA,99000,40,24000), (Scott,Finance,NY,83000,36,19000), (Jen,Finance,NY,79000,53,15000), (Jeff,Marketing,CA,80000,25,18000), (Kumar,Marketing,NY,91000,50,21000))\r\n",
       "df: org.apache.spark.sql.DataFrame = [employee_name: string, department: string ... 4 more fields]\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "val simpleData = Seq((\"James\",\"Sales\",\"NY\",90000,34,10000),\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000),\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n",
    "    (\"Raman\",\"Finance\",\"CA\",99000,40,24000),\n",
    "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000),\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000),\n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000)\n",
    "  )\n",
    "val df = simpleData.toDF(\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3d6673",
   "metadata": {},
   "source": [
    "## groupBy y agregados en las columnas del DataFrame\n",
    "Hagamos el ```groupBy()``` en la columna 'department' del DataFrame y luego encontremos la suma del salario de cada departamento usando la función de agregada ```sum()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d899f3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|department|sum(salary)|\n",
      "+----------+-----------+\n",
      "|Sales     |257000     |\n",
      "|Finance   |351000     |\n",
      "|Marketing |171000     |\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"department\").sum(\"salary\").show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28515e0b",
   "metadata": {},
   "source": [
    "Del mismo modo, podemos calcular el número de empleados de cada departamento mediante ```count()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78490603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|department|count|\n",
      "+----------+-----+\n",
      "|     Sales|    3|\n",
      "|   Finance|    4|\n",
      "| Marketing|    2|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"department\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d557e5c",
   "metadata": {},
   "source": [
    "Calcular el salario mínimo de cada departamento mediante ```min()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4eae6707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|department|min(salary)|\n",
      "+----------+-----------+\n",
      "|     Sales|      81000|\n",
      "|   Finance|      79000|\n",
      "| Marketing|      80000|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"department\").min(\"salary\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad058a51",
   "metadata": {},
   "source": [
    "Calcular el salario máximo de cada departamento mediante ```max()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06bb3367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|department|max(salary)|\n",
      "+----------+-----------+\n",
      "|     Sales|      90000|\n",
      "|   Finance|      99000|\n",
      "| Marketing|      91000|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"department\").max(\"salary\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbe27d5",
   "metadata": {},
   "source": [
    "Calcule el salario medio de cada departamento utilizando ```avg()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "baea8713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+\n",
      "|department|      avg(salary)|\n",
      "+----------+-----------------+\n",
      "|     Sales|85666.66666666667|\n",
      "|   Finance|          87750.0|\n",
      "| Marketing|          85500.0|\n",
      "+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"department\").avg(\"salary\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2b3fff",
   "metadata": {},
   "source": [
    "Calcule el salario medio de cada departamento utilizando ```mean()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c5abb30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+\n",
      "|department|      avg(salary)|\n",
      "+----------+-----------------+\n",
      "|     Sales|85666.66666666667|\n",
      "|   Finance|          87750.0|\n",
      "| Marketing|          85500.0|\n",
      "+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"department\").mean(\"salary\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae49cdd",
   "metadata": {},
   "source": [
    "## groupBy y aggregate en múltiples columnas del DataFrame\n",
    "Del mismo modo, también podemos ejecutar ```groupBy()``` y ```aggregate()``` en dos o más columnas del DataFrame, el siguiente ejemplo hace group by en el 'department', 'state' y hace ```sum()``` en las columnas de 'salary' y 'bonus'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dba14eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----------+----------+\n",
      "|department|state|sum(salary)|sum(bonus)|\n",
      "+----------+-----+-----------+----------+\n",
      "|Sales     |NY   |176000     |30000     |\n",
      "|Sales     |CA   |81000      |23000     |\n",
      "|Finance   |CA   |189000     |47000     |\n",
      "|Finance   |NY   |162000     |34000     |\n",
      "|Marketing |NY   |91000      |21000     |\n",
      "|Marketing |CA   |80000      |18000     |\n",
      "+----------+-----+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//GroupBy en multiples columnas\n",
    "df.groupBy(\"department\",\"state\")\n",
    "    .sum(\"salary\",\"bonus\")\n",
    "    .show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0498464",
   "metadata": {},
   "source": [
    "Del mismo modo, podemos ejecutar ```gruopby()``` y ```aggregate()``` en dos o más columnas para otras funciones de agregación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31807ff5",
   "metadata": {},
   "source": [
    "## Ejecutar más agregados a la vez\n",
    "Usando la función de agregación ```agg()``` podemos calcular muchas agregaciones a la vez en una sola sentencia usando las funciones de agregación de Spark SQL ```sum()```, ```avg()```, ```min()```, ```max()``` ```mean()``` etc... Para usarlas, debemos importar ```import org.apache.spark.sql.functions._```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "801959dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------------+---------+---------+\n",
      "|department|sum_salary|avg_salary       |sum_bonus|max_bonus|\n",
      "+----------+----------+-----------------+---------+---------+\n",
      "|Sales     |257000    |85666.66666666667|53000    |23000    |\n",
      "|Finance   |351000    |87750.0          |81000    |24000    |\n",
      "|Marketing |171000    |85500.0          |39000    |21000    |\n",
      "+----------+----------+-----------------+---------+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions._\r\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "df.groupBy(\"department\")\n",
    "    .agg(\n",
    "      sum(\"salary\").as(\"sum_salary\"),\n",
    "      avg(\"salary\").as(\"avg_salary\"),\n",
    "      sum(\"bonus\").as(\"sum_bonus\"),\n",
    "      max(\"bonus\").as(\"max_bonus\"))\n",
    "    .show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a672240b",
   "metadata": {},
   "source": [
    "## Uso del filtro en los datos agregados\n",
    "De forma similar a la cláusula \"HAVING\" de SQL, en Spark DataFrame podemos utilizar la función ```where()``` o ```filter()``` para filtrar las filas de los datos agregados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91ebfd00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------------+---------+---------+\n",
      "|department|sum_salary|avg_salary       |sum_bonus|max_bonus|\n",
      "+----------+----------+-----------------+---------+---------+\n",
      "|Sales     |257000    |85666.66666666667|53000    |23000    |\n",
      "|Finance   |351000    |87750.0          |81000    |24000    |\n",
      "+----------+----------+-----------------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"department\")\n",
    "    .agg(\n",
    "      sum(\"salary\").as(\"sum_salary\"),\n",
    "      avg(\"salary\").as(\"avg_salary\"),\n",
    "      sum(\"bonus\").as(\"sum_bonus\"),\n",
    "      max(\"bonus\").as(\"max_bonus\"))\n",
    "    .where(col(\"sum_bonus\") >= 50000)\n",
    "    .show(false)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
