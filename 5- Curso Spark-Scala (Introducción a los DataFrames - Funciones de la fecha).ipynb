{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e492fc81",
   "metadata": {},
   "source": [
    "# Ejemplos de Funciones de Fecha de Spark\n",
    "A continuación se muestran los ejemplos más utilizados de las funciones de fecha.\n",
    "\n",
    "## ```current_date()``` y ```date_format()```\n",
    "Veremos cómo obtener la fecha actual y convertir la fecha en un formato específico usando ```date_format()``` con un ejemplo de Scala. El siguiente ejemplo analiza la fecha y la convierte del formato 'aaaa-dd-mm' al formato 'MM-dd-aaaa'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3e35ce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://ALC-1NJW5D3.usersad.everis.int:4042\n",
       "SparkContext available as 'sc' (version = 3.3.0, master = local[*], app id = local-1656925484014)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/04 11:04:58 WARN ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped\n",
      "+------------+----------+----------+\n",
      "|current_date|     Input|    format|\n",
      "+------------+----------+----------+\n",
      "|  2022-07-04|2019-01-23|01-23-2019|\n",
      "+------------+----------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions._\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "Seq((\"2019-01-23\"))\n",
    "  .toDF(\"Input\")\n",
    "  .select( \n",
    "    current_date()as(\"current_date\"), \n",
    "    col(\"Input\"), \n",
    "    date_format(col(\"Input\"), \"MM-dd-yyyy\").as(\"format\") \n",
    "  ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711fa178",
   "metadata": {},
   "source": [
    "## ```to_date()```\n",
    "El siguiente ejemplo convierte una cadena en formato de fecha 'MM/dd/aaaa' a un DateType 'aaaa-MM-dd' usando ```to_date()``` con el ejemplo de Scala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78fbadc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|     Input|   to_date|\n",
      "+----------+----------+\n",
      "|04/13/2019|2019-04-13|\n",
      "+----------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions._\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "Seq((\"04/13/2019\"))\n",
    "   .toDF(\"Input\")\n",
    "  .select( col(\"Input\"), \n",
    "           to_date(col(\"Input\"), \"MM/dd/yyyy\").as(\"to_date\") \n",
    "   ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb05190",
   "metadata": {},
   "source": [
    "## ```datediff()```\n",
    "El siguiente ejemplo devuelve la diferencia entre dos fechas utilizando ```datediff()``` con el ejemplo de Scala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca609329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+----+\n",
      "|     input|current_date()|diff|\n",
      "+----------+--------------+----+\n",
      "|2019-01-23|    2022-07-04|1258|\n",
      "|2019-06-24|    2022-07-04|1106|\n",
      "|2019-09-20|    2022-07-04|1018|\n",
      "+----------+--------------+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions._\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "Seq((\"2019-01-23\"),(\"2019-06-24\"),(\"2019-09-20\"))\n",
    "   .toDF(\"input\")\n",
    "   .select( col(\"input\"), current_date(), \n",
    "       datediff(current_date(),col(\"input\")).as(\"diff\") \n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63cab14",
   "metadata": {},
   "source": [
    "## ```months_between()```\n",
    "El siguiente ejemplo devuelve los meses entre dos fechas utilizando ```months_between()``` con el lenguaje Scala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c22450c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------+--------------+\n",
      "|      date|current_date()|datediff|months_between|\n",
      "+----------+--------------+--------+--------------+\n",
      "|2019-01-23|    2022-07-04|    1258|   41.38709677|\n",
      "|2019-06-24|    2022-07-04|    1106|   36.35483871|\n",
      "|2019-09-20|    2022-07-04|    1018|   33.48387097|\n",
      "+----------+--------------+--------+--------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions._\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "Seq((\"2019-01-23\"),(\"2019-06-24\"),(\"2019-09-20\"))\n",
    "   .toDF(\"date\")\n",
    "  .select( col(\"date\"), current_date(), \n",
    "       datediff(current_date(),col(\"date\")).as(\"datediff\"), \n",
    "       months_between(current_date(),col(\"date\")).as(\"months_between\")\n",
    "   ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba04d8bd",
   "metadata": {},
   "source": [
    "## ```trunc()```\n",
    "El siguiente ejemplo trunca la fecha en una unidad especificada usando ```trunc()``` con el lenguaje Scala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ac4f635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+----------+-----------+\n",
      "|     input|Month_Trunc|Month_Year|Month_Trunc|\n",
      "+----------+-----------+----------+-----------+\n",
      "|2019-01-23| 2019-01-01|2019-01-01| 2019-01-01|\n",
      "|2019-06-24| 2019-06-01|2019-01-01| 2019-06-01|\n",
      "|2019-09-20| 2019-09-01|2019-01-01| 2019-09-01|\n",
      "+----------+-----------+----------+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions._\r\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "Seq((\"2019-01-23\"),(\"2019-06-24\"),(\"2019-09-20\"))\n",
    "    .toDF(\"input\")\n",
    "    .select( col(\"input\"), \n",
    "          trunc(col(\"input\"),\"Month\").as(\"Month_Trunc\"), \n",
    "          trunc(col(\"input\"),\"Year\").as(\"Month_Year\"), \n",
    "          trunc(col(\"input\"),\"Month\").as(\"Month_Trunc\") \n",
    "     ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34b6590",
   "metadata": {},
   "source": [
    "## ```add_months()```, ```date_add()```, ```date_sub()```\n",
    "Aquí estamos sumando y restando la fecha y el mes de una entrada dada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea2f6165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+----------+\n",
      "|     input|add_months|sub_months|  date_add|  date_sub|\n",
      "+----------+----------+----------+----------+----------+\n",
      "|2019-01-23|2019-04-23|2018-10-23|2019-01-27|2019-01-19|\n",
      "|2019-06-24|2019-09-24|2019-03-24|2019-06-28|2019-06-20|\n",
      "|2019-09-20|2019-12-20|2019-06-20|2019-09-24|2019-09-16|\n",
      "+----------+----------+----------+----------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions._\r\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "Seq((\"2019-01-23\"),(\"2019-06-24\"),(\"2019-09-20\")).toDF(\"input\")\n",
    "  .select( col(\"input\"), \n",
    "      add_months(col(\"input\"),3).as(\"add_months\"), \n",
    "      add_months(col(\"input\"),-3).as(\"sub_months\"), \n",
    "      date_add(col(\"input\"),4).as(\"date_add\"), \n",
    "      date_sub(col(\"input\"),4).as(\"date_sub\") \n",
    "   ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d52671",
   "metadata": {},
   "source": [
    "## ```year()```, ```month()```, ```dayofweek()```\n",
    "## ```dayofmonth()```, ```dayofyear()```, ```next_day()```, ```weekofyear()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e103c5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----+---------+----------+---------+----------+----------+\n",
      "|     input|year|month|dayofweek|dayofmonth|dayofyear|  next_day|weekofyear|\n",
      "+----------+----+-----+---------+----------+---------+----------+----------+\n",
      "|2019-01-23|2019|    1|        4|        23|       23|2019-01-27|         4|\n",
      "|2019-06-24|2019|    6|        2|        24|      175|2019-06-30|        26|\n",
      "|2019-09-20|2019|    9|        6|        20|      263|2019-09-22|        38|\n",
      "+----------+----+-----+---------+----------+---------+----------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions._\r\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "Seq((\"2019-01-23\"),(\"2019-06-24\"),(\"2019-09-20\"))\n",
    "  .toDF(\"input\")\n",
    "  .select( col(\"input\"), year(col(\"input\")).as(\"year\"), \n",
    "       month(col(\"input\")).as(\"month\"), \n",
    "       dayofweek(col(\"input\")).as(\"dayofweek\"), \n",
    "       dayofmonth(col(\"input\")).as(\"dayofmonth\"), \n",
    "       dayofyear(col(\"input\")).as(\"dayofyear\"), \n",
    "       next_day(col(\"input\"),\"Sunday\").as(\"next_day\"), \n",
    "       weekofyear(col(\"input\")).as(\"weekofyear\") \n",
    "   ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4749d30",
   "metadata": {},
   "source": [
    "# Ejemplos de Funciones Timestamp de Spark\n",
    "A continuación se muestran los ejemplos más utilizados de las funciones Timestamp.\n",
    "\n",
    "## ```current_timestamp()```\n",
    "Devuelve la marca de tiempo actual en el formato por defecto de Spark **yyyy-MM-dd HH:mm:ss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "523e8119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-----------------------+\n",
      "|seq|current_date|current_timestamp      |\n",
      "+---+------------+-----------------------+\n",
      "|1  |2022-07-04  |2022-07-04 11:15:37.499|\n",
      "+---+------------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions._\r\n",
       "df: org.apache.spark.sql.DataFrame = [seq: int]\r\n",
       "curDate: org.apache.spark.sql.DataFrame = [seq: int, current_date: date ... 1 more field]\r\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val df = Seq((1)).toDF(\"seq\")\n",
    "val curDate = df.withColumn(\"current_date\",current_date().as(\"current_date\"))\n",
    " .withColumn(\"current_timestamp\",current_timestamp().as(\"current_timestamp\"))\n",
    "curDate.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fc5796",
   "metadata": {},
   "source": [
    "## ```to_timestamp()```\n",
    "Convierte la cadena de tiempo a un formato de tipo Timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f14e7f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-----------------------+\n",
      "|input_timestamp        |datetype_timestamp     |\n",
      "+-----------------------+-----------------------+\n",
      "|07-01-2019 12 01 19 406|2019-07-01 12:01:19.406|\n",
      "|06-24-2019 12 01 19 406|2019-06-24 12:01:19.406|\n",
      "|11-16-2019 16 44 55 406|2019-11-16 16:44:55.406|\n",
      "|11-16-2019 16 50 59 406|2019-11-16 16:50:59.406|\n",
      "+-----------------------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions._\r\n",
       "dfDate: org.apache.spark.sql.DataFrame = [input_timestamp: string]\r\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val dfDate = Seq((\"07-01-2019 12 01 19 406\"),\n",
    "    (\"06-24-2019 12 01 19 406\"),\n",
    "    (\"11-16-2019 16 44 55 406\"),\n",
    "    (\"11-16-2019 16 50 59 406\")).toDF(\"input_timestamp\")\n",
    "\n",
    "  dfDate.withColumn(\"datetype_timestamp\",\n",
    "          to_timestamp(col(\"input_timestamp\"),\"MM-dd-yyyy HH mm ss SSS\"))\n",
    "    .show(false)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304bb7fa",
   "metadata": {},
   "source": [
    "## ```hour()```, ```Minute()``` y ```second()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0440a9d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+----+------+------+\n",
      "|input_timestamp        |hour|minute|second|\n",
      "+-----------------------+----+------+------+\n",
      "|2019-07-01 12:01:19.000|12  |1     |19    |\n",
      "|2019-06-24 12:01:19.000|12  |1     |19    |\n",
      "|2019-11-16 16:44:55.406|16  |44    |55    |\n",
      "|2019-11-16 16:50:59.406|16  |50    |59    |\n",
      "+-----------------------+----+------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions._\r\n",
       "df: org.apache.spark.sql.DataFrame = [input_timestamp: string]\r\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val df = Seq((\"2019-07-01 12:01:19.000\"),\n",
    "    (\"2019-06-24 12:01:19.000\"),\n",
    "    (\"2019-11-16 16:44:55.406\"),\n",
    "    (\"2019-11-16 16:50:59.406\")).toDF(\"input_timestamp\")\n",
    "\n",
    "  df.withColumn(\"hour\", hour(col(\"input_timestamp\")))\n",
    "    .withColumn(\"minute\", minute(col(\"input_timestamp\")))\n",
    "    .withColumn(\"second\", second(col(\"input_timestamp\")))\n",
    "    .show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dce33ff",
   "metadata": {},
   "source": [
    "# Conclusión\n",
    "Se ha consolidado la lista completa de Timestamp y fecha de Spark con una descripción y ejemplo de algunas de uso común."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
