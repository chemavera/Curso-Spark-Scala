{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "499f4a60",
   "metadata": {},
   "source": [
    "# Crear DataFrames en Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6363d17",
   "metadata": {},
   "source": [
    "En Spark, los métodos `createDataFrame()` y `toDF()` se utilizan para crear un DataFrame manualmente, utilizando estos métodos se puede crear un DataFrame de Spark a partir de objetos de datos RDD, DataFrame, Dataset, List, Seq ya existentes, aquí voy a examinar estos con ejemplos de Scala.\n",
    "\n",
    "También puedes crear un DataFrame a partir de diferentes fuentes como texto, CSV, JSON, XML, Parquet, Avro, ORC, archivos binarios, tablas RDBMS, Hive, HBase, y muchos más."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656c8c27",
   "metadata": {},
   "source": [
    "DataFrame es una colección distribuida de datos organizada en columnas con nombre. Es conceptualmente equivalente a una tabla en una base de datos relacional o a un marco de datos en R/Python, pero con optimizaciones más ricas bajo el capó. Los DataFrames pueden construirse a partir de una amplia gama de fuentes como: archivos de datos estructurados, tablas en Hive, bases de datos externas o RDDs existentes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f680c974",
   "metadata": {},
   "source": [
    "En primer lugar, vamos a importar los implícitos de Spark que necesitemos para nuestros ejemplos (por ejemplo, cuando queramos utilizar la función `.toDF()`) y crear los datos de muestra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f50525a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://ALC-1NJW5D3.usersad.everis.int:4040\n",
       "SparkContext available as 'sc' (version = 3.3.0, master = local[*], app id = local-1656667285616)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/01 11:21:32 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\r\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@318c2d9d\r\n",
       "import spark.implicits._\r\n",
       "columns: Seq[String] = List(language, users_count)\r\n",
       "data: Seq[(String, String)] = List((Java,20000), (Python,100000), (Scala,3000))\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession.builder()\n",
    "                        .master(\"local[1]\")\n",
    "                        .appName(\"SparkByExamples.com\")\n",
    "                        .getOrCreate()\n",
    "\n",
    "import spark.implicits._\n",
    "val columns = Seq(\"language\",\"users_count\")\n",
    "val data = Seq((\"Java\", \"20000\"), (\"Python\", \"100000\"), (\"Scala\", \"3000\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea41524",
   "metadata": {},
   "source": [
    "## 1. Spark crea un DataFrame a partir de un RDD\n",
    "Una forma fácil de crear manualmente un Spark DataFrame es a partir de un RDD existente. Primero, vamos a crear un RDD a partir de una colección Seq llamando a parallelize().\n",
    "\n",
    "Utilizaré este objeto RDD para todos nuestros ejemplos a continuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2e860aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[(String, String)] = ParallelCollectionRDD[0] at parallelize at <console>:29\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = spark.sparkContext.parallelize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8209f6e0",
   "metadata": {},
   "source": [
    "### 1.1 Uso de la función toDF()\n",
    "Una vez que tenemos un RDD, vamos a utilizar `toDF()` para crear DataFrame en Spark. Por defecto, crea los nombres de las columnas como \"_1\" y \"_2\" ya que tenemos dos columnas para cada fila."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d769188c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      "\n",
      "+------+------+\n",
      "|    _1|    _2|\n",
      "+------+------+\n",
      "|  Java| 20000|\n",
      "|Python|100000|\n",
      "| Scala|  3000|\n",
      "+------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dfFromRDD1: org.apache.spark.sql.DataFrame = [_1: string, _2: string]\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfFromRDD1 = rdd.toDF()\n",
    "\n",
    "dfFromRDD1.printSchema()\n",
    "dfFromRDD1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4ae0d0",
   "metadata": {},
   "source": [
    "Dado que el RDD no tiene esquema, sin nombres de columna y tipo de datos, la conversión de RDD a DataFrame le da nombres de columna por defecto como _1, _2 y así sucesivamente y el tipo de datos como String. Utiliza DataFrame `printSchema()` para imprimir el esquema en la consola."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccd042a",
   "metadata": {},
   "source": [
    "`toDF()` tiene otra firma para asignar un nombre de columna, esta toma un número variable de argumentos para los nombres de columna como se muestra a continuación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd95f32",
   "metadata": {},
   "source": [
    "Produce la siguiente salida. Recuerde que aquí acabamos de asignar los nombres de las columnas, pero se toman todos los tipos de datos como cadenas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d66ba3d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/01 11:21:36 WARN ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped\n",
      "root\n",
      " |-- language: string (nullable = true)\n",
      " |-- users_count: string (nullable = true)\n",
      "\n",
      "+--------+-----------+\n",
      "|language|users_count|\n",
      "+--------+-----------+\n",
      "|    Java|      20000|\n",
      "|  Python|     100000|\n",
      "|   Scala|       3000|\n",
      "+--------+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dfFromRDD1: org.apache.spark.sql.DataFrame = [language: string, users_count: string]\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfFromRDD1 = rdd.toDF(\"language\",\"users_count\")\n",
    "\n",
    "dfFromRDD1.printSchema()\n",
    "dfFromRDD1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b32495",
   "metadata": {},
   "source": [
    "Por defecto, el tipo de datos de estas columnas se asigna a String. Podemos cambiar este comportamiento suministrando el esquema, donde podemos especificar un nombre de columna, tipo de datos y nullable para cada campo/columna."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906ef5d9",
   "metadata": {},
   "source": [
    "### 1.2 Usando Spark createDataFrame() desde SparkSession\n",
    "Usar `createDataFrame()` desde SparkSession es otra forma de crear y toma el objeto rdd como argumento y lo encadena con `toDF()` para especificar nombres a las columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d53cb8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|language|users_count|\n",
      "+--------+-----------+\n",
      "|    Java|      20000|\n",
      "|  Python|     100000|\n",
      "|   Scala|       3000|\n",
      "+--------+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dfFromRDD2: org.apache.spark.sql.DataFrame = [language: string, users_count: string]\r\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfFromRDD2 = spark.createDataFrame(rdd).toDF(columns:_*)\n",
    "\n",
    "dfFromRDD2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd481da1",
   "metadata": {},
   "source": [
    "### 1.3 Utilización de createDataFrame() con el tipo Row\n",
    "`createDataFrame()` tiene otra firma que toma el tipo `RDD[Row]` y el esquema para los nombres de las columnas como argumentos. Para usar esto primero necesitamos convertir nuestro objeto \"rdd\" de `RDD[T]` a `RDD[Row]` y definir un esquema usando StructType & StructField."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de064c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|language| users|\n",
      "+--------+------+\n",
      "|    Java| 20000|\n",
      "|  Python|100000|\n",
      "|   Scala|  3000|\n",
      "+--------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types.{StringType, StructField, StructType}\r\n",
       "import org.apache.spark.sql.Row\r\n",
       "schema: org.apache.spark.sql.types.StructType = StructType(StructField(language,StringType,true),StructField(users,StringType,true))\r\n",
       "rowRDD: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[10] at map at <console>:35\r\n",
       "dfFromRDD3: org.apache.spark.sql.DataFrame = [language: string, users: string]\r\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types.{StringType, StructField, StructType}\n",
    "import org.apache.spark.sql.Row\n",
    "val schema = StructType( Array(\n",
    "                 StructField(\"language\", StringType,true),\n",
    "                 StructField(\"users\", StringType,true)\n",
    "             ))\n",
    "val rowRDD = rdd.map(attributes => Row(attributes._1, attributes._2))\n",
    "val dfFromRDD3 = spark.createDataFrame(rowRDD,schema)\n",
    "\n",
    "dfFromRDD3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eedbcc6",
   "metadata": {},
   "source": [
    "## 2. Crear un Spark DataFrame a partir de una colección List y Seq\n",
    "En esta sección, veremos varias aproximaciones para crear Spark DataFrame a partir de la colección `Seq[T]` o `List[T]`. Estos ejemplos serían similares a lo que hemos visto en la sección anterior con RDD, pero utilizamos el objeto \"data\" en lugar del objeto \"rdd\".\n",
    "\n",
    "### 2.1 Utilizar toDF() sobre una colección List o Seq\n",
    "`toDF()` sobre una colección (Seq, List) crea un DataFrame. asegúrate de `import spark.implicits._` para usar `toDF()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9dffe740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|    _1|    _2|\n",
      "+------+------+\n",
      "|  Java| 20000|\n",
      "|Python|100000|\n",
      "| Scala|  3000|\n",
      "+------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\r\n",
       "dfFromData1: org.apache.spark.sql.DataFrame = [_1: string, _2: string]\r\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "val dfFromData1 = data.toDF() \n",
    "\n",
    "dfFromData1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db9eb89",
   "metadata": {},
   "source": [
    "### 2.2 Uso de createDataFrame() desde SparkSession\n",
    "Llamando a `createDataFrame()` desde `SparkSession` es otra forma de crear y toma como argumento un objeto de colección (Seq o List) y lo encadena con `toDF()` para especificar nombres a las columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1516bcfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|language|users_count|\n",
      "+--------+-----------+\n",
      "|    Java|      20000|\n",
      "|  Python|     100000|\n",
      "|   Scala|       3000|\n",
      "+--------+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dfFromData2: org.apache.spark.sql.DataFrame = [language: string, users_count: string]\r\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//From Data (USING createDataFrame)\n",
    "var dfFromData2 = spark.createDataFrame(data).toDF(columns:_*)\n",
    "\n",
    "dfFromData2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fb25bb",
   "metadata": {},
   "source": [
    "### 2.3 Uso de createDataFrame() con el tipo Row\n",
    "`createDataFrame()` tiene otra firma en Spark que toma el `util.List` de tipo Row y el esquema para los nombres de las columnas como argumentos. Para usar esto primero necesitamos `importar scala.collection.JavaConversions._`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24fefbfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|language| users|\n",
      "+--------+------+\n",
      "|    Java| 20000|\n",
      "|  Python|100000|\n",
      "|   Scala|  3000|\n",
      "+--------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import scala.collection.JavaConversions._\r\n",
       "rowData: Seq[org.apache.spark.sql.Row] = List([Java,20000], [Python,100000], [Scala,3000])\r\n",
       "dfFromData3: org.apache.spark.sql.DataFrame = [language: string, users: string]\r\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.collection.JavaConversions._\n",
    "//From Data (USING createDataFrame and Adding schema using StructType)\n",
    "val rowData= Seq(Row(\"Java\", \"20000\"), \n",
    "               Row(\"Python\", \"100000\"), \n",
    "               Row(\"Scala\", \"3000\"))\n",
    "var dfFromData3 = spark.createDataFrame(rowData,schema)\n",
    "\n",
    "dfFromData3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1dc8f5",
   "metadata": {},
   "source": [
    "## 3. Crear un Spark DataFrame a partir de CSV\n",
    "En todos los ejemplos anteriores, has aprendido a Spark a crear DataFrame a partir de RDD y objetos de recolección de datos. En tiempo real estos son menos utilizados, en esta y en las siguientes secciones, aprenderás a crear DataFrame a partir de fuentes de datos como CSV, texto, JSON, Avro, etc.\n",
    "\n",
    "Spark proporciona por defecto una API para leer archivos con delimitadores como comas, tuberías, tabuladores y también proporciona varias opciones de manejo con cabecera, sin cabecera, comillas dobles, tipos de datos, etc.\n",
    "\n",
    "Para un ejemplo detallado, consulte crear un DataFrame a partir de un archivo CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ef1d138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-------+\n",
      "|sepal.length|sepal.width|petal.length|petal.width|variety|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "|         5.1|        3.5|         1.4|         .2| Setosa|\n",
      "|         4.9|          3|         1.4|         .2| Setosa|\n",
      "|         4.7|        3.2|         1.3|         .2| Setosa|\n",
      "|         4.6|        3.1|         1.5|         .2| Setosa|\n",
      "|           5|        3.6|         1.4|         .2| Setosa|\n",
      "|         5.4|        3.9|         1.7|         .4| Setosa|\n",
      "|         4.6|        3.4|         1.4|         .3| Setosa|\n",
      "|           5|        3.4|         1.5|         .2| Setosa|\n",
      "|         4.4|        2.9|         1.4|         .2| Setosa|\n",
      "|         4.9|        3.1|         1.5|         .1| Setosa|\n",
      "|         5.4|        3.7|         1.5|         .2| Setosa|\n",
      "|         4.8|        3.4|         1.6|         .2| Setosa|\n",
      "|         4.8|          3|         1.4|         .1| Setosa|\n",
      "|         4.3|          3|         1.1|         .1| Setosa|\n",
      "|         5.8|          4|         1.2|         .2| Setosa|\n",
      "|         5.7|        4.4|         1.5|         .4| Setosa|\n",
      "|         5.4|        3.9|         1.3|         .4| Setosa|\n",
      "|         5.1|        3.5|         1.4|         .3| Setosa|\n",
      "|         5.7|        3.8|         1.7|         .3| Setosa|\n",
      "|         5.1|        3.8|         1.5|         .3| Setosa|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df2: org.apache.spark.sql.DataFrame = [sepal.length: string, sepal.width: string ... 3 more fields]\r\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df2 = spark.read.option(\"header\", \"true\").csv(\"data/iris.csv\")\n",
    "\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664a433d",
   "metadata": {},
   "source": [
    "## 4. Creación a partir de un archivo de texto (TXT)\n",
    "Aquí veremos cómo crear a partir de un archivo TXT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a842e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "val df2 = spark.read\n",
    ".text(\"/src/resources/file.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5d7b4d",
   "metadata": {},
   "source": [
    "## 5. Creando desde un archivo JSON\n",
    "Aquí veremos cómo crear a partir de un archivo JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2236ebb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "val df2 = spark.read\n",
    ".json(\"/src/resources/file.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7801f4",
   "metadata": {},
   "source": [
    "## 6. Creando desde un archivo XML\n",
    "Para crear DataFrame parseando XML, debemos utilizar el DataSource \"com.databricks.spark.xml\" spark-xml api de Databricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c992471d",
   "metadata": {},
   "outputs": [],
   "source": [
    "val df = spark.read\n",
    "      .format(\"com.databricks.spark.xml\")\n",
    "      .option(\"rowTag\", \"person\")\n",
    "      .xml(\"src/main/resources/persons.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f6b00e",
   "metadata": {},
   "source": [
    "## 7. Creación a partir de Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c68fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "val hiveContext = new org.apache.spark.sql.hive.HiveContext(spark.sparkContext)\n",
    "val hiveDF = hiveContext.sql(“select * from emp”)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4b27f5",
   "metadata": {},
   "source": [
    "## 8. Spark crea un DataFrame a partir de una base de datos RDBMS\n",
    "### 8.a) Desde una tabla Mysql\n",
    "Asegúrate de que tienes la librería MySQL como dependencia en tu archivo pom.xml o los jars de MySQL en tu classpath."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d639ebb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "val df_mysql = spark.read.format(“jdbc”)\n",
    "   .option(“url”, “jdbc:mysql://localhost:port/db”)\n",
    "   .option(“driver”, “com.mysql.jdbc.Driver”)\n",
    "   .option(“dbtable”, “tablename”) \n",
    "   .option(“user”, “user”) \n",
    "   .option(“password”, “password”) \n",
    "   .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa0ffa9",
   "metadata": {},
   "source": [
    "### 8.1 Desde la tabla DB2\n",
    "Asegúrese de que tiene la biblioteca DB2 como dependencia en su archivo pom.xml o los jars DB2 en su classpath."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017d1d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "val df_db2 = spark.read.format(“jdbc”)\n",
    "   .option(“url”, “jdbc:db2://localhost:50000/dbname”)\n",
    "   .option(“driver”, “com.ibm.db2.jcc.DB2Driver”)\n",
    "   .option(“dbtable”, “tablename”) \n",
    "   .option(“user”, “user”) \n",
    "   .option(“password”, “password”) \n",
    "   .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6081227",
   "metadata": {},
   "source": [
    "Del mismo modo, podemos crear DataFrame en Spark a partir de la mayoría de las bases de datos relacionales que no he cubierto aquí y que dejaré que exploren.\n",
    "\n",
    "## 9. Crear DataFrame a partir de una tabla HBase\n",
    "Para crear un DataFrame en Spark a partir de una tabla HBase, debemos utilizar el DataSource definido en los conectores HBase de Spark. Por ejemplo, utilizar el DataSource \"`org.apache.spark.sql.execution.datasources.hbase`\" de Hortonworks o utilizar \"`org.apache.hadoop.hbase.spark`\" del conector HBase de Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e587b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val hbaseDF = sparkSession.read\n",
    "      .options(Map(HBaseTableCatalog.tableCatalog -> catalog))\n",
    "      .format(\"org.apache.spark.sql.execution.datasources.hbase\")\n",
    "      .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3944adab",
   "metadata": {},
   "source": [
    "## 10. Otras fuentes (Avro, Parquet, Kafka)\n",
    "También podemos crear DataFrame a partir de Avro, Parquet, HBase y leer datos de Kafka"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
