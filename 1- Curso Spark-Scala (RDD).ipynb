{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70a3d02e",
   "metadata": {},
   "source": [
    "## ¿Qué es RDD (Resilient Distributed Dataset)?\n",
    "RDD (Resilient Distributed Dataset) es una estructura de datos fundamental de Spark y es la principal abstracción de datos en Apache Spark y el Spark Core. Los RDDs son colecciones distribuidas de objetos inmutables y tolerantes a fallos, lo que significa que una vez que se crea un RDD no se puede cambiar. Cada conjunto de datos en RDD se divide en particiones lógicas, que pueden ser calculadas en diferentes nodos del clúster. \n",
    "\n",
    "En otras palabras, los RDDs son una colección de objetos similar a las colecciones en Scala, con la diferencia de que los RDDs se computan en varias JVMs dispersas en múltiples servidores físicos también llamados nodos en un cluster mientras que una colección en Scala vive en una única JVM.\n",
    "\n",
    "Además, los RDDs proporcionan una abstracción de datos de partición y distribución de los datos que está diseñada para ejecutar cálculos en paralelo en varios nodos, mientras que al hacer transformaciones en RDD la mayoría de las veces no tenemos que preocuparnos del paralelismo como proporciona Spark por defecto.\n",
    "\n",
    "Este tutorial de Apache Spark RDD describe las operaciones básicas disponibles en los RDDs, como ``map``, ``filter``, y ``persist``, etc. usando un ejemplo de Scala. Además, este tutorial también explica las funciones RDD de pares que operan sobre RDDs de pares clave-valor como ``groupByKey`` y ``join`` etc.\n",
    "\n",
    "## Ventajas de los RDDs\n",
    "* Procesamiento en memoria\n",
    "* Inmutabilidad\n",
    "* Tolerancia a los fallos\n",
    "* Evolución perezosa\n",
    "* Particionamiento\n",
    "* Paralelización\n",
    "\n",
    "## Limitaciones\n",
    "Los RDDs de Spark no son muy adecuados para aplicaciones que realizan actualizaciones en el almacén de estado, como los sistemas de almacenamiento de una aplicación web. Para estas aplicaciones, es más eficiente utilizar sistemas que realicen el tradicional registro de actualizaciones y checkpointing de datos, como las bases de datos. El objetivo de los RDD es proporcionar un modelo de programación eficiente para el análisis por lotes y dejar estas aplicaciones asíncronas.\n",
    "\n",
    "Un RDD puede estar presente en un solo SparkContext y el RDD puede tener un nombre y un identificador único (id).\n",
    "\n",
    "## Creación de RDDs\n",
    "Los RDD's se crean principalmente de dos maneras diferentes, primero paralelizando una colección existente y segundo referenciando un conjunto de datos en un sistema de almacenamiento externo (HDFS, S3 y muchos más). \n",
    "\n",
    "Antes de ver los ejemplos, primero vamos a inicializar SparkSession utilizando el método de patrón constructor definido en la clase SparkSession. Mientras se inicializa, necesitamos proporcionar el nombre del maestro y de la aplicación como se muestra a continuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3d5ff33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://ALC-1NJW5D3.usersad.everis.int:4041\n",
       "SparkContext available as 'sc' (version = 3.3.0, master = local[*], app id = local-1657109660950)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/06 14:14:26 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\r\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@65c6986a\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/06 14:14:31 WARN ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped\r\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession.builder()\n",
    "      .master(\"local[1]\")\n",
    "      .appName(\"SparkByExamples.com\")\n",
    "      .getOrCreate()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473958fc",
   "metadata": {},
   "source": [
    "![Esquema RDD](images/rdd-creation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c1bc12",
   "metadata": {},
   "source": [
    "## Uso de ``sparkContext.parallelize()``\n",
    "``sparkContext.parallelize()`` se utiliza para paralelizar una colección existente en su programa controlador. Este es un método básico para crear RDD y se utiliza principalmente mientras POC o prototipos y se requiere que todos los datos estén presentes en el programa controlador antes de crear RDD por lo tanto no es más utilizado para aplicaciones de producción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1cfe4af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataSeq: Seq[(String, Int)] = List((Java,20000), (Python,100000), (Scala,3000))\r\n",
       "rdd: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[0] at parallelize at <console>:27\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Create RDD from parallelize    \n",
    "val dataSeq = Seq((\"Java\", 20000), (\"Python\", 100000), (\"Scala\", 3000))   \n",
    "val rdd=spark.sparkContext.parallelize(dataSeq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b5e6e8",
   "metadata": {},
   "source": [
    "Para las aplicaciones de producción, la mayoría de las veces creamos RDD utilizando sistemas de almacenamiento externos como HDFS, S3, HBase, etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0902fe43",
   "metadata": {},
   "source": [
    "## Uso de ``sparkContext.wholeTextFiles()``\n",
    "La función ``wholeTextFiles()`` devuelve un PairRDD cuya clave es la ruta del archivo y cuyo valor es el contenido del mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74cb3fdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd1: org.apache.spark.rdd.RDD[(String, String)] = /data/iris.csv MapPartitionsRDD[2] at wholeTextFiles at <console>:26\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Reads entire file into a RDD as single record.\n",
    "val rdd1 = spark.sparkContext.wholeTextFiles(\"/data/iris.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e82b5e",
   "metadata": {},
   "source": [
    "## Usando ``sparkContext.emptyRDD``\n",
    "Usando el método ``emptyRDD()`` en sparkContext podemos crear un RDD sin datos. Este método crea un RDD vacío sin partición."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e5d54fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdde: org.apache.spark.rdd.RDD[Nothing] = EmptyRDD[3] at emptyRDD at <console>:26\r\n",
       "rddeString: org.apache.spark.rdd.RDD[String] = EmptyRDD[4] at emptyRDD at <console>:27\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Creates empty RDD with no partition    \n",
    "val rdde = spark.sparkContext.emptyRDD // creates EmptyRDD[0]\n",
    "val rddeString = spark.sparkContext.emptyRDD[String] // creates EmptyRDD[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2eb574",
   "metadata": {},
   "source": [
    "## Crear un RDD vacío con partición\n",
    "Algunas veces podemos necesitar escribir un RDD vacío en archivos por partición, En este caso, debes crear un RDD vacío con partición."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2587bc13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd2: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[5] at parallelize at <console>:26\r\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Create empty RDD with partition\n",
    "val rdd2 = spark.sparkContext.parallelize(Seq.empty[String])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17053b82",
   "metadata": {},
   "source": [
    "## RDD Paralelizar y reparticionar\n",
    "Cuando utilizamos los métodos ``parallelize()``, ``textFile()`` o ``wholeTextFiles()`` de ``SparkContext`` para iniciar el RDD, éste divide automáticamente los datos en particiones según la disponibilidad de recursos. \n",
    "\n",
    "### ``getNumPartitions`` \n",
    "* Devuelve el número de particiones en que se divide nuestro conjunto de datos. Cualquier transformación aplicada en el RDD se ejecuta de forma paralela. Spark ejecutará una tarea por cada partición del cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da658768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recuento inicial de particiones: 8\n"
     ]
    }
   ],
   "source": [
    "println(\"Recuento inicial de particiones: \" + rdd2.getNumPartitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c33b98",
   "metadata": {},
   "source": [
    "### Establecer el paralelismo manualmente \n",
    "* También podemos establecer un número de particiones manualmente, todo lo que necesitamos es pasar un número de particiones como segundo parámetro a estas funciones, por ejemplo ``sparkContext.parallelize(dataSeq, 10))``. \n",
    "\n",
    "### Repalalizar usando repartition y coalesce \n",
    "* Algunas veces podemos necesitar reparticionar el RDD, Spark proporciona dos maneras de reparticionar; primero usando el método ``repartition()`` que baraja los datos de todos los nodos también llamado full shuffle y el segundo método ``coalesce()`` que baraja los datos de los nodos mínimos, por ejemplo si tienes datos en 4 particiones y haciendo coalesce(2) mueves los datos de sólo 2 nodos. \n",
    "\n",
    "    Ambas funciones toman el número de particiones para reparticionar la rdd como se muestra a continuación. Ten en cuenta que el método ``<a href=\"https://sparkbyexamples.com/spark/spark-repartition-vs-coalesce/\">repartición()</a>`` es una operación muy cara ya que baraja los datos de todos los nodos de un cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc853e9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reparRdd: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[9] at repartition at <console>:25\r\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val reparRdd = rdd2.repartition(4)\n",
    "\n",
    "println(\"Recuento de la repartición: \" + reparRdd.getNumPartitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6e89be",
   "metadata": {},
   "source": [
    "**Nota:** los métodos ``repartition()`` o ``coalesce()`` también devuelven un nuevo RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1089d1",
   "metadata": {},
   "source": [
    "## Operaciones con RDDs\n",
    "**Transformaciones RDD** - Las transformaciones son operaciones perezosas, en lugar de actualizar un RDD, estas operaciones devuelven otro RDD.\n",
    "**Acciones RDD** - Operaciones que desencadenan cálculos y devuelven valores RDD.\n",
    "\n",
    "### Transformaciones RDD\n",
    "Las transformaciones sobre RDD de Spark devuelven otro RDD y las transformaciones son perezosas, lo que significa que no se ejecutan hasta que se llame a una acción sobre el RDD. Algunas transformaciones sobre RDD's son ``flatMap``, ``map``, ``reduceByKey``, ``filter``, ``sortByKey`` y devuelven un nuevo RDD en lugar de actualizar el actual.\n",
    "\n",
    "Transformaciones usando el ejemplo de conteo de palabras. La siguiente imagen muestra las diferentes transformaciones RDD que vamos a utilizar.\n",
    "\n",
    "![Recuento de palabras transformaciones RDD 'spark'](images/rdd-transformation-work-count.png)\n",
    "\n",
    "En primer lugar, crearemos un RDD leyendo un archivo de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b00d1c2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.rdd.RDD\r\n",
       "rdd3: org.apache.spark.rdd.RDD[String] = data/test.txt MapPartitionsRDD[11] at textFile at <console>:27\r\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.rdd.RDD\n",
    "\n",
    "val rdd3:RDD[String] = spark.sparkContext.textFile(\"data/test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d83e8f5",
   "metadata": {},
   "source": [
    "**flatMap** - La transformación ``flatMap()`` aplana el RDD después de aplicar la función y devuelve un nuevo RDD. En el siguiente ejemplo, primero divide cada registro por espacios en un RDD y finalmente lo aplana. El RDD resultante consta de una sola palabra en cada registro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da805193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd4: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[12] at flatMap at <console>:26\r\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd4 = rdd3.flatMap(f=>f.split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d831531",
   "metadata": {},
   "source": [
    "**La transformación map** - ``map()`` se utiliza para aplicar cualquier operación compleja como añadir una columna, actualizar una columna, etc., la salida de las transformaciones map siempre tendrá el mismo número de registros que la entrada.\n",
    "\n",
    "En nuestro ejemplo de recuento de palabras, estamos añadiendo una nueva columna con valor 1 para cada palabra, el resultado del RDD es PairRDDFunctions que contiene pares clave-valor, palabra de tipo String como Key y 1 de tipo Int como valor. Para su comprensión, he definido la variable rdd3 con el tipo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08316c7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd5: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[13] at map at <console>:26\r\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd5:RDD[(String,Int)]= rdd4.map(m=>(m,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7f60d8",
   "metadata": {},
   "source": [
    "**filter** - La transformación ``filter()`` se utiliza para filtrar los registros de un RDD. En nuestro ejemplo estamos filtrando todas las palabras que empiezan por \"a\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b53a8108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd6: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[14] at filter at <console>:26\r\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd6 = rdd5.filter(a=> a._1.startsWith(\"a\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b318705e",
   "metadata": {},
   "source": [
    "**reduceByKey** - ``reduceByKey()`` fusiona los valores de cada clave con la función especificada. En nuestro ejemplo, reduce la cadena de palabras aplicando la función de suma sobre el valor. El resultado de nuestro RDD contiene palabras únicas y su recuento. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "faabda3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd7: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[15] at reduceByKey at <console>:26\r\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd7 = rdd6.reduceByKey(_ + _)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7b3488",
   "metadata": {},
   "source": [
    "**sortByKey** – ``sortByKey()`` se utiliza para ordenar los elementos del RDD según la clave. En nuestro ejemplo, primero, convertimos ``RDD[(String,Int])`` a ``RDD[(Int, String])`` utilizando la transformación map y aplicamos sortByKey que, idealmente, ordena sobre un valor entero. Y finalmente, foreach con declaraciones println devuelve todas las palabras en el RDD y su conteo como par clave-valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42b15a7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd8: org.apache.spark.rdd.RDD[(Int, String)] = ShuffledRDD[19] at sortByKey at <console>:26\r\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd8 = rdd7.map(a=>(a._2,a._1)).sortByKey()\n",
    "//Print rdd6 result to console\n",
    "rdd8.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06f72bf",
   "metadata": {},
   "source": [
    "Última declaración foreach en rdd imprime el recuento de cada palabra.\n",
    "\n",
    "## Acciones RDD\n",
    "La acción de operación RDD devuelve los valores brutos de un RDD. En otras palabras, cualquier función RDD que devuelva valores no ``RDD[T]`` se considera una acción. \n",
    "\n",
    "En este tutorial de Spark RDD Action, seguiremos utilizando nuestro ejemplo de conteo de palabras, la última declaración ``foreach()`` es una acción que devuelve todos los datos de un RDD y los imprime en una consola. veamos algunas operaciones de acción más en nuestro ejemplo de conteo de palabras.\n",
    "\n",
    "**count** - Devuelve el número de registros en un RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ddb9895b",
   "metadata": {},
   "outputs": [],
   "source": [
    "//Action - count\n",
    "println(\"Cuenta : \" + rdd8.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fb4f2f",
   "metadata": {},
   "source": [
    "**first** - Devuelve el primer registro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67393804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "firstRec: (Int, String) = (27,anyone)\r\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Action - first\n",
    "val firstRec = rdd8.first()\n",
    "println(\"First Record : \" + firstRec._1 + \",\" + firstRec._2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012b43ac",
   "metadata": {},
   "source": [
    "**max** - Devuelve el registro máximo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac38d7df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datMax: (Int, String) = (27,at)\r\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Action - max\n",
    "val datMax = rdd8.max()\n",
    "println(\"Max Record : \" + datMax._1 + \",\" + datMax._2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf37cc7",
   "metadata": {},
   "source": [
    "**reduce** - Reduce los registros a uno solo, podemos usar esto para contar o sumar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "277b60fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "totalWordCount: (Int, String) = (108,anyone)\r\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Action - reduce\n",
    "val totalWordCount = rdd8.reduce((a,b) => (a._1+b._1,a._2))\n",
    "println(\"dataReduce Record : \" + totalWordCount._1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5baeb3ad",
   "metadata": {},
   "source": [
    "**take** - Devuelve el registro especificado como argumento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e149c4e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data3: Array[(Int, String)] = Array((27,anyone), (27,at), (27,and))\r\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Action - take\n",
    "val data3 = rdd8.take(3)\n",
    "data3.foreach(f=>{\n",
    "  println(\"data3 Key:\"+ f._1 +\", Value:\"+f._2)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5300bc8",
   "metadata": {},
   "source": [
    "**collect** - Devuelve todos los datos del RDD como una matriz. Tenga cuidado al utilizar esta acción cuando trabaje con un RDD enorme con millones y miles de millones de datos, ya que puede quedarse sin memoria en el controlador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a36ae0e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: Array[(Int, String)] = Array((27,anyone), (27,at), (27,and), (27,anywhere))\r\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Action - collect\n",
    "val data = rdd8.collect()\n",
    "data.foreach(f=>{\n",
    "  println(\"Key:\"+ f._1 +\", Value:\"+f._2)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6272f5",
   "metadata": {},
   "source": [
    "**saveAsTextFile** - Usando la acción saveAsTestFile, podemos escribir el RDD en un archivo de texto.\n",
    "\n",
    "``rdd8.saveAsTextFile(\"saves/wordCount\")``\n",
    "\n",
    "## Tipos de RDD\n",
    "* **PairRDDFunctions o PairRDD** - Pair RDD es un par clave-valor Este es el tipo de RDD más utilizado, \n",
    "\n",
    "* **ShuffledRDD**\n",
    "\n",
    "* **DobleRDD**\n",
    "\n",
    "* **SequenceFileRDD** \n",
    "\n",
    "* **HadoopRDD**\n",
    "\n",
    "* **ParallelCollectionRDD** \n",
    "\n",
    "## Operaciones de barajado\n",
    "Shuffling es un mecanismo que Spark utiliza para redistribuir los datos entre diferentes ejecutores e incluso entre máquinas. Spark shuffling se activa cuando realizamos ciertas operaciones de transformación como ``gropByKey()``, ``reduceByKey()``, ``join()`` en RDDS\n",
    "\n",
    "Spark Shuffle es una operación costosa ya que implica lo siguiente:\n",
    "* I/O en disco\n",
    "* Implica la serialización y deserialización de datos\n",
    "* I/O de red\n",
    "\n",
    "Al crear un RDD, Spark no almacena necesariamente los datos de todas las claves de una partición, ya que en el momento de la creación no podemos establecer la clave del conjunto de datos.\n",
    "\n",
    "Por lo tanto, cuando ejecutamos la operación reduceByKey() para agregar los datos en las claves, Spark hace lo siguiente. necesita primero ejecutar tareas para recoger todos los datos de todas las particiones y por ejemplo, cuando realizamos la operación reduceByKey(), Spark hace lo siguiente:\n",
    "\n",
    "* Spark ejecuta primero tareas de mapeo en todas las particiones que agrupan todos los valores de una misma clave.\n",
    "* Los resultados de las tareas de mapa se guardan en memoria.\n",
    "* Cuando los resultados no caben en la memoria, Spark almacena los datos en un disco.\n",
    "* Spark baraja los datos mapeados entre las particiones, algunas veces también almacena los datos barajados en un disco para reutilizarlos cuando necesite recalcular.\n",
    "* Ejecuta la recolección de basura\n",
    "* Finalmente ejecuta las tareas de reducción en cada partición basándose en la clave.\n",
    "\n",
    "Spark RDD activa el shuffle y repartition para varias operaciones como ``repartition()`` y ``coalesce()``, ``groupByKey()``, ``reduceByKey()``, ``cogroup()`` y ``join()`` pero no ``countByKey()`` .\n",
    "\n",
    "## Tamaño de la partición aleatoria y rendimiento\n",
    "Según el tamaño de su conjunto de datos, el número de núcleos y la memoria, el barajado de Spark puede beneficiar o perjudicar sus trabajos. Cuando se trata de una menor cantidad de datos, normalmente se deben reducir las particiones de barajado, de lo contrario se terminará con muchos archivos particionados con menos número de registros en cada partición, lo que resulta en la ejecución de muchas tareas con menos datos para procesar.\n",
    "\n",
    "Por otro lado, cuando tiene demasiados datos y tiene menos número de particiones resulta en menos tareas que se ejecutan por más tiempo y algunas veces también puede obtener un error de memoria.\n",
    "\n",
    "Conseguir el tamaño correcto de la partición aleatoria es siempre complicado y requiere muchas ejecuciones con diferentes valores para conseguir el número optimizado. Esta es una de las propiedades clave que hay que buscar cuando se tienen problemas de rendimiento en los trabajos de Spark.\n",
    "\n",
    "## Persistencia RDD\n",
    "Spark Cache y Persist son técnicas de optimización para mejorar el rendimiento de los trabajos RDD que son iterativos e interactivos. En esta sección del Tutorial Spark RDD, explicaré cómo utilizar los métodos persist() y cache() en RDD con ejemplos.\n",
    "\n",
    "Aunque Spark proporciona un cálculo 100 veces más rápido que los trabajos tradicionales de Map Reduce, si no se han diseñado los trabajos para reutilizar los cálculos repetitivos se verá una degradación en el rendimiento cuando se trate de miles de millones o trillones de datos. Por lo tanto, tenemos que ver los cálculos y utilizar técnicas de optimización como una de las formas de mejorar el rendimiento.\n",
    "\n",
    "Utilizando los métodos ``cache()`` y ``persist()``, Spark proporciona un mecanismo de optimización para almacenar los cálculos intermedios de un RDD para que puedan ser reutilizados en acciones posteriores.\n",
    "\n",
    "Cuando se persiste o se almacena en caché un RDD, cada nodo trabajador almacena sus datos particionados en memoria o disco y los reutiliza en otras acciones sobre ese RDD. Además, los datos persistentes de Spark en los nodos son tolerantes a los fallos, lo que significa que si se pierde alguna partición, se volverá a calcular automáticamente utilizando las transformaciones originales que la crearon.\n",
    "\n",
    "## Ventajas de la persistencia de RDD\n",
    "* **Coste eficiente** - Los cálculos de Spark son muy caros, por lo que la reutilización de los cálculos se utiliza para ahorrar costes.\n",
    "* **Tiempo eficiente** - Reutilizar los cálculos repetidos ahorra mucho tiempo.\n",
    "* **Tiempo de ejecución** - Ahorra tiempo de ejecución del trabajo lo que nos permite realizar más trabajos en el mismo cluster.\n",
    "\n",
    "## RDD Cache\n",
    "Spark RDD ``<strong>cache()</strong>`` guarda por defecto el cálculo del RDD en el nivel de almacenamiento aa, lo que significa que almacenará los datos en el montón de la JVM como objetos no serializados.\n",
    "\n",
    "El método Spark ``cache()`` en la clase RDD llama internamente al método ``persist()`` que a su vez utiliza ``sparkSession.sharedState.cacheManager.cacheQuery`` para almacenar en caché el conjunto de resultados del RDD.\n",
    "\n",
    "## RDD Persist\n",
    "El método Spark ``persist()`` se utiliza para almacenar el RDD en uno de los niveles de almacenamiento ``<strong>MEMORY_ONLY</strong>``,``<strong>MEMORY_AND_DISK</strong>``, ``<strong>MEMORY_ONLY_SER</strong>``, ``<strong>MEMORY_AND_DISK_SER</strong>``, ``<strong>DISK_ONLY</strong>``, ``<strong>MEMORY_ONLY_2</strong>``,``<strong>MEMORY_AND_DISK_2</strong>`` y más.\n",
    "\n",
    "Spark ``persist()`` tiene dos firmas, la primera no toma ningún argumento que por defecto lo guarda en ``<strong>MEMORY_ONLY</strong>`` nivel de almacenamiento y la segunda firma que toma StorageLevel como argumento para guardarlo en diferentes niveles de almacenamiento.\n",
    "\n",
    "## RDD Unpersist\n",
    "Spark monitoriza automáticamente cada una de las llamadas a ``persist()`` y ``cache()`` que realizas y comprueba el uso en cada nodo y elimina los datos persistidos si no se utilizan o utilizando el algoritmo de mínimo uso reciente (LRU). También se puede eliminar manualmente utilizando el método ``unpersist()``. ``unpersist()`` marca el RDD como no persistente, y elimina todos los bloques de la memoria y el disco.\n",
    "\n",
    "## Niveles de almacenamiento de persistencia\n",
    "Todos los diferentes niveles de almacenamiento que soporta Spark están disponibles en la clase ``org.apache.spark.storage.StorageLevel``. El nivel de almacenamiento define cómo y dónde almacenar el RDD.\n",
    "\n",
    "``<strong>MEMORY_ONLY</strong>`` - Este es el comportamiento por defecto del método RDD ``cache()`` y almacena el RDD como objetos deserializados en la memoria de la JVM. Cuando no hay suficiente memoria disponible no se guardará en el RDD de algunas particiones y éstas se volverán a calcular como y cuando se requiera. Esto toma más almacenamiento pero se ejecuta más rápido ya que toma pocos ciclos de CPU para leer de la memoria.\n",
    "\n",
    "``<strong>MEMORY_ONLY_SER</strong>`` - Es lo mismo que ``MEMORY_ONLY`` pero con la diferencia de que almacena los RDD como objetos serializados en la memoria de la JVM. Ocupa menos memoria (espacio-eficiente) que MEMORY_ONLY ya que guarda los objetos como serializados y toma unos pocos ciclos de CPU adicionales para deserializar.\n",
    "\n",
    "``<strong>MEMORY_ONLY_2</strong>`` - Igual que el nivel de almacenamiento ``MEMORY_ONLY`` pero replicando cada partición en dos nodos del cluster.\n",
    "\n",
    "``<strong>MEMORY_ONLY_SER_2</strong>`` - Igual que el nivel de almacenamiento ``MEMORY_ONLY_SER`` pero replicando cada partición en dos nodos del cluster.\n",
    "\n",
    "``<strong>MEMORY_AND_DISK</strong>`` - En este nivel de almacenamiento, el RDD será almacenado en la memoria de la JVM como objetos deserializados. Cuando el almacenamiento requerido es mayor que la memoria disponible, almacena algunas de las particiones sobrantes en el disco y lee los datos del disco cuando se requiere. Es más lento ya que hay I/O involucrada.\n",
    "\n",
    "``<strong>MEMORY_AND_DISK_SER</strong>`` - Es el mismo nivel de almacenamiento que ``MEMORY_AND_DISK`` con la diferencia de que serializa los objetos RDD en memoria y en disco cuando no hay espacio disponible.\n",
    "\n",
    "``<strong>MEMORY_AND_DISK_2</strong>`` - Igual que el nivel de almacenamiento ``MEMORY_AND_DISK`` pero replicando cada partición en dos nodos del cluster.\n",
    "\n",
    "``<strong>MEMORY_AND_DISK_SER_2</strong>`` - Igual que el nivel de almacenamiento ``MEMORY_AND_DISK_SER`` pero replicando cada partición en dos nodos del cluster.\n",
    "\n",
    "``<strong>DISK_ONLY</strong>`` - En este nivel de almacenamiento, el RDD se almacena sólo en el disco y el tiempo de cálculo de la CPU es alto al estar involucrada la I/O.\n",
    "\n",
    "``<strong>DISK_ONLY_2</strong>`` - Igual que el nivel de almacenamiento ``DISK_ONLY`` pero replicando cada partición en dos nodos del cluster.\n",
    "\n",
    "## Variables compartidas de Spark\n",
    "En esta sección del tutorial de Spark RDD, vamos a aprender cuáles son los diferentes tipos de variables compartidas de Spark y cómo se utilizan en las transformaciones de Spark.\n",
    "\n",
    "Cuando Spark ejecuta una transformación utilizando las operaciones ``map()`` o ``reduce()``, ejecuta las transformaciones en un nodo remoto utilizando las variables que se envían con las tareas y estas variables no se envían de vuelta al controlador de Spark, por lo que no hay capacidad para reutilizar y compartir las variables entre las tareas. Las variables compartidas de Spark resuelven este problema utilizando las dos técnicas siguientes. Spark proporciona dos tipos de variables compartidas.\n",
    "\n",
    "* Variables de difusión (variable compartida de sólo lectura)\n",
    "* Variables acumuladoras (variables compartidas actualizables)\n",
    "\n",
    "## Variables de difusión\n",
    "Las variables de difusión son variables compartidas de sólo lectura que se almacenan en caché y están disponibles en todos los nodos de un clúster para que las tareas puedan acceder a ellas o utilizarlas. En lugar de enviar estos datos junto con cada tarea, Spark distribuye las variables de difusión a la máquina utilizando algoritmos de difusión eficientes para reducir los costes de comunicación.\n",
    "\n",
    "Uno de los mejores casos de uso de Spark RDD Broadcast es utilizarlo con datos de búsqueda, por ejemplo, código postal, estado, país, etc.\n",
    "\n",
    "Cuando se ejecuta un trabajo Spark RDD que tiene las variables Broadcast definidas y utilizadas, Spark hace lo siguiente.\n",
    "\n",
    "* Spark rompe el trabajo en etapas que han distribuido la baraja y las acciones se ejecutan con en la etapa.\n",
    "* Las etapas posteriores también se dividen en tareas\n",
    "* Spark difunde los datos comunes (reutilizables) que necesitan las tareas dentro de cada etapa.\n",
    "* Los datos difundidos se almacenan en caché en formato serializado y se deserializan antes de ejecutar cada tarea\n",
    "\n",
    "La difusión de Spark se crea mediante el método ``broadcast(v)`` de la clase SparkContext. Este método toma el argumento v que se quiere emitir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd0a412a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "broadcastVar: org.apache.spark.broadcast.Broadcast[Array[Int]] = Broadcast(12)\r\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val broadcastVar = sc.broadcast(Array(0, 1, 2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d97340dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res9: Array[Int] = Array(0, 1, 2, 3)\r\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broadcastVar.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b1f4b2",
   "metadata": {},
   "source": [
    "Tenga en cuenta que las variables de difusión no se envían a los ejecutores con la llamada ``sc.broadcast(variable)``, sino que se enviarán a los ejecutores cuando se utilicen por primera vez.\n",
    "\n",
    "## Acumuladores\n",
    "Los acumuladores de Spark son otro tipo de variables compartidas que sólo se \"añaden\" mediante una operación asociativa y conmutativa y se utilizan para realizar contadores (similares a los contadores de Map-reduce) u operaciones de suma.\n",
    "\n",
    "Spark soporta por defecto la creación de un acumulador de cualquier tipo numérico y proporciona la capacidad de añadir tipos de acumuladores personalizados. Los programadores pueden crear los siguientes acumuladores\n",
    "\n",
    "* acumuladores con nombre\n",
    "* acumuladores sin nombre\n",
    "\n",
    "Cuando se crea un acumulador con nombre, se puede ver en la interfaz web de Spark en la pestaña \"Acumulador\". En esta pestaña, verá dos tablas; la primera tabla \"acumulable\" - consiste en todas las variables del acumulador con nombre y sus valores. Y en la segunda tabla \"Tareas\" - valor de cada acumulador modificado por una tarea.\n",
    "\n",
    "Mientras que los acumuladores sin nombre no se muestran en la interfaz web de Spark, para todos los propósitos prácticos es sugerible usar acumuladores con nombre.\n",
    "\n",
    "Las variables de los acumuladores se crean usando ``SparkContext.longAccumulator(v)``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "93659765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accum: org.apache.spark.util.LongAccumulator = LongAccumulator(id: 251, name: Some(SumAccumulator), value: 0)\r\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val accum = sc.longAccumulator(\"SumAccumulator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c513c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.parallelize(Array(1, 2, 3)).foreach(x => accum.add(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ff0b444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res11: Long = 6\r\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accum.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2968ab23",
   "metadata": {},
   "source": [
    "Spark proporciona por defecto métodos acumuladores para los tipos long, double y collection. Todos estos métodos están presentes en la clase SparkContext y devuelven ``LongAccumulator``, ``DoubleAccumulator`` y ``CollectionAccumulator`` respectivamente.\n",
    "\n",
    "## Creación de RDD a partir de DataFrame y viceversa\n",
    "Aunque tenemos API's más avanzadas sobre RDD, a menudo necesitaremos convertir DataFrame a RDD o RDD a DataFrame. A continuación hay varios ejemplos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5842a96c",
   "metadata": {},
   "source": [
    "``//Converts RDD to DataFrame\n",
    "val dfFromRDD1 = rdd.toDF()\n",
    "//Converts RDD to DataFrame with column names\n",
    "val dfFromRDD2 = rdd.toDF(\"col1\",\"col2\")\n",
    "//using createDataFrame() - Convert DataFrame to RDD\n",
    "val df = spark.createDataFrame(rdd).toDF(\"col1\",\"col2\")\n",
    "//Convert RDD to Dataset\n",
    "val ds = spark.createDataset(rdd)\n",
    "//Convert DataFrame to RDD\n",
    "val rdd = df.rdd``"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
