{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54850d47",
   "metadata": {},
   "source": [
    "# Cómo pivotar y despivotar un DataFrame de Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd459386",
   "metadata": {},
   "source": [
    "La función de Spark, ```pivot()```, se utiliza para pivotar/rotar los datos de una columna del DataFrame/Dataset en múltiples columnas (transformar fila en columna) y ```unpivot()``` se utiliza para transformarlos de nuevo (transformar columnas en filas).\n",
    "\n",
    "```Pivot()``` es una agregación en la que uno de los valores de las columnas de agrupación se transpone en columnas individuales con datos distintos.\n",
    "\n",
    "Vamos a crear un DataFrame para trabajar con él."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c081935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://ALC-1NJW5D3.usersad.everis.int:4041\n",
       "SparkContext available as 'sc' (version = 3.3.0, master = local[*], app id = local-1656923599632)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/04 10:33:34 WARN ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped\n",
      "+-------+------+-------+\n",
      "|Product|Amount|Country|\n",
      "+-------+------+-------+\n",
      "| Banana|  1000|    USA|\n",
      "|Carrots|  1500|    USA|\n",
      "|  Beans|  1600|    USA|\n",
      "| Orange|  2000|    USA|\n",
      "| Orange|  2000|    USA|\n",
      "| Banana|   400|  China|\n",
      "|Carrots|  1200|  China|\n",
      "|  Beans|  1500|  China|\n",
      "| Orange|  4000|  China|\n",
      "| Banana|  2000| Canada|\n",
      "|Carrots|  2000| Canada|\n",
      "|  Beans|  2000| Mexico|\n",
      "+-------+------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "data: Seq[(String, Int, String)] = List((Banana,1000,USA), (Carrots,1500,USA), (Beans,1600,USA), (Orange,2000,USA), (Orange,2000,USA), (Banana,400,China), (Carrots,1200,China), (Beans,1500,China), (Orange,4000,China), (Banana,2000,Canada), (Carrots,2000,Canada), (Beans,2000,Mexico))\r\n",
       "import spark.sqlContext.implicits._\r\n",
       "df: org.apache.spark.sql.DataFrame = [Product: string, Amount: int ... 1 more field]\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = Seq((\"Banana\",1000,\"USA\"), (\"Carrots\",1500,\"USA\"), (\"Beans\",1600,\"USA\"),\n",
    "      (\"Orange\",2000,\"USA\"),(\"Orange\",2000,\"USA\"),(\"Banana\",400,\"China\"),\n",
    "      (\"Carrots\",1200,\"China\"),(\"Beans\",1500,\"China\"),(\"Orange\",4000,\"China\"),\n",
    "      (\"Banana\",2000,\"Canada\"),(\"Carrots\",2000,\"Canada\"),(\"Beans\",2000,\"Mexico\"))\n",
    "\n",
    "import spark.sqlContext.implicits._\n",
    "\n",
    "val df = data.toDF(\"Product\",\"Amount\",\"Country\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c853b8c",
   "metadata": {},
   "source": [
    "## Pivotar Spark DataFrame\n",
    "Spark SQL proporciona la función ```pivot()``` para girar los datos de una columna en múltiples columnas (transponer fila a columna). Se trata de una agregación en la que uno de los valores de las columnas de agrupación se transpone en columnas individuales con datos distintos. A partir del DataFrame anterior, para obtener la cantidad total exportada a cada país de cada producto se hará la agrupación por 'Product', el pivote por 'Country', y la suma de 'Amount'.\n",
    "\n",
    "Esto transpondrá los países de las filas del DataFrame a las columnas. Cuando los datos no están presentes, se representan como nulos por defecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3efff84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----+------+----+\n",
      "|Product|Canada|China|Mexico| USA|\n",
      "+-------+------+-----+------+----+\n",
      "| Orange|  null| 4000|  null|4000|\n",
      "|  Beans|  null| 1500|  2000|1600|\n",
      "| Banana|  2000|  400|  null|1000|\n",
      "|Carrots|  2000| 1200|  null|1500|\n",
      "+-------+------+-----+------+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pivotDF: org.apache.spark.sql.DataFrame = [Product: string, Canada: bigint ... 3 more fields]\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pivotDF = df.groupBy(\"Product\").pivot(\"Country\").sum(\"Amount\")\n",
    "pivotDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc74172",
   "metadata": {},
   "source": [
    "## Mejora del rendimiento de Pivot en Spark 2.0\n",
    "El rendimiento de Spark 2.0 en adelante ha sido mejorado en Pivot, sin embargo, si estás usando una versión inferior; ten en cuenta que el pivote es una operación muy costosa por lo tanto, se recomienda proporcionar los datos de la columna (si se conocen) como un argumento a la función como se muestra a continuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dc25612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----+------+------+\n",
      "|Product| USA|China|Canada|Mexico|\n",
      "+-------+----+-----+------+------+\n",
      "| Orange|4000| 4000|  null|  null|\n",
      "|  Beans|1600| 1500|  null|  2000|\n",
      "| Banana|1000|  400|  2000|  null|\n",
      "|Carrots|1500| 1200|  2000|  null|\n",
      "+-------+----+-----+------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "countries: Seq[String] = List(USA, China, Canada, Mexico)\r\n",
       "pivotDF: org.apache.spark.sql.DataFrame = [Product: string, USA: bigint ... 3 more fields]\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val countries = Seq(\"USA\",\"China\",\"Canada\",\"Mexico\")\n",
    "val pivotDF = df.groupBy(\"Product\").pivot(\"Country\", countries).sum(\"Amount\")\n",
    "pivotDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8279a0da",
   "metadata": {},
   "source": [
    "Otro enfoque es hacer una agregación en dos fases. Spark 2.0 utiliza esta implementación para mejorar el rendimiento Spark-13749"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "132cc3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----+------+----+\n",
      "|Product|Canada|China|Mexico| USA|\n",
      "+-------+------+-----+------+----+\n",
      "| Orange|  null| 4000|  null|4000|\n",
      "|  Beans|  null| 1500|  2000|1600|\n",
      "| Banana|  2000|  400|  null|1000|\n",
      "|Carrots|  2000| 1200|  null|1500|\n",
      "+-------+------+-----+------+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pivotDF: org.apache.spark.sql.DataFrame = [Product: string, Canada: bigint ... 3 more fields]\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pivotDF = df.groupBy(\"Product\",\"Country\")\n",
    "      .sum(\"Amount\")\n",
    "      .groupBy(\"Product\")\n",
    "      .pivot(\"Country\")\n",
    "      .sum(\"sum(Amount)\")\n",
    "pivotDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7a3b91",
   "metadata": {},
   "source": [
    "Los dos ejemplos anteriores devuelven el mismo resultado pero con mejor rendimiento.\n",
    "## Unpivot Spark DataFrame\n",
    "Unpivot es una operación inversa, que podemos lograr girando los valores de las columnas en los valores de las filas. Spark SQL no tiene la función unpivot, por lo que utilizará la función ```stack()```. El código siguiente convierte los países de las columnas en filas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53430a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Product|Country|Total|\n",
      "+-------+-------+-----+\n",
      "| Orange|  China| 4000|\n",
      "|  Beans|  China| 1500|\n",
      "|  Beans| Mexico| 2000|\n",
      "| Banana| Canada| 2000|\n",
      "| Banana|  China|  400|\n",
      "|Carrots| Canada| 2000|\n",
      "|Carrots|  China| 1200|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "unPivotDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Product: string, Country: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Unpivot\n",
    "val unPivotDF = pivotDF.select($\"Product\",\n",
    "expr(\"stack(3, 'Canada', Canada, 'China', China, 'Mexico', Mexico) as (Country,Total)\"))\n",
    ".where(\"Total is not null\")\n",
    "unPivotDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd82a81",
   "metadata": {},
   "source": [
    "## Transposición o pivote sin agregación\n",
    "¿Podemos hacer la transposición o el pivote de Spark DataFrame sin agregación?\n",
    "\n",
    "Por supuesto que se puede, pero desafortunadamente, no se puede lograr usando la función Pivot. Sin embargo, pivotar o transponer la estructura del DataFrame sin agregación de filas a columnas y de columnas a filas se puede hacer fácilmente usando Spark y Scala hack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "819fcf98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|COLUMN_NAME|VALUE|\n",
      "+-----------+-----+\n",
      "|       col1| val1|\n",
      "|       col2| val2|\n",
      "|       col3| val3|\n",
      "|       col4| val4|\n",
      "|       col5| val5|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [COLUMN_NAME: string, VALUE: string]\r\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = Seq(\n",
    "  (\"col1\", \"val1\"),\n",
    "  (\"col2\", \"val2\"),\n",
    "  (\"col3\", \"val3\"),\n",
    "  (\"col4\", \"val4\"),\n",
    "  (\"col5\", \"val5\")\n",
    ").toDF(\"COLUMN_NAME\", \"VALUE\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43a52b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+----+\n",
      "|col1|col2|col3|col4|col5|\n",
      "+----+----+----+----+----+\n",
      "|val1|val2|val3|val4|val5|\n",
      "+----+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy()\n",
    "  .pivot(\"COLUMN_NAME\").agg(first(\"VALUE\"))\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b038395b",
   "metadata": {},
   "source": [
    "Si el DataFrame es realmente tan pequeño como en el ejemplo, podemos recogerlo como Map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "496d7c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map(col3 -> val3, col2 -> val2, col5 -> val5, col1 -> val1, col4 -> val4)"
     ]
    },
    {
     "data": {
      "text/plain": [
       "map: scala.collection.immutable.Map[String,String] = Map(col3 -> val3, col2 -> val2, col5 -> val5, col1 -> val1, col4 -> val4)\r\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val map = df.as[(String,String)].collect().toMap\n",
    "\n",
    "print(map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4f72e7",
   "metadata": {},
   "source": [
    "## Conclusión\n",
    "Hemos visto cómo Pivotar DataFrame (transponer fila a columna) con el ejemplo de scala y Unpivot de nuevo usando las funciones SQL de Spark. Y también hemos visto cómo los cambios de Spark 2.0 mejoran el rendimiento al hacer la agregación en dos fases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
